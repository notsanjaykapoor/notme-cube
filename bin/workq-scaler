#!/usr/bin/env python

import functools
import json
import os
import signal
import sys

sys.path.append(".")

import dot_init

import asyncio
import click
import sqlmodel

import log
import models
import services.clusters.requests
import services.database
import services.hetzner.servers
import services.workers

logger = log.init("api")

shutdown: bool = False

workers_set = set()
workers_map = {}

def coro(f):
    @functools.wraps(f)
    def wrapper(*args, **kwargs):
        return asyncio.run(f(*args, **kwargs))

    return wrapper


@click.group()
def cli():
    pass


def signal_handler(signum, frame):
    global shutdown
    shutdown = True
    logger.info("[workq-scaler] signal handler ... preparing to shutdown")


@click.command()
@click.option('--db-uri', default="", required=False, help="db uri string, e.g. postgresql://postgres:postgres@postgres-dev:5433/db_src")
@click.option('--interval', default=3, required=False, help="")
@coro
async def run(db_uri: str, interval: int) -> dict:
    """
    The scaler is responsible for listening for incoming cluster requests and managing cluster machine size based on those requests.

    Recovery conditions:

    When the scaler is processing a cluster request and its shutdown, the up/down request is left in an unknown state.  The recovery
    for this situation is to find any requests in the processing state (there should be at most 1), determine how many machines are 
    running and set this value as the cluster size_has.  The scaler is then resumed to process the request as normal.
    """
    db_uri = db_uri or os.environ.get("DATABASE_URL")

    if not db_uri:
        raise ValueError("db_uri is invalid or missing")

    # install signal handler
    signal.signal(signal.SIGINT, signal_handler)

    logger.info("[workq-scaler] starting")

    with services.database.session.get() as db_session:
        requests = services.clusters.requests.get_processing_all(db_session=db_session)

        if requests:
            _startup_recovery(db_session=db_session, requests=requests)

    while not shutdown:
        with services.database.session.get() as db_session:
            request = services.clusters.requests.get_pending_any(db_session=db_session)

            if request:
                cluster = services.clusters.get_by_id(
                    db_session=db_session,
                    id=request.cluster_id,
                )

                logger.info(f"[workq-scaler] cluster '{cluster.name}' request {request.id} state '{request.state}' ask {request.size_ask} has {cluster.size_has}")

                services.clusters.requests.process(
                    db_session=db_session,
                    request=request,
                )
            else:
                await asyncio.sleep(interval)

    logger.info("[workq-scaler] exiting")


def _startup_recovery(db_session: sqlmodel.Session, requests: list[models.ClusterRequest]) -> int:
    for request in requests:
        cluster = services.clusters.get_by_id(
            db_session=db_session,
            id=request.cluster_id,
        )

        logger.info(f"[workq-scaler] cluster '{cluster.name}' request {request.id} state '{request.state}' ask {request.size_ask} has {cluster.size_has} - recovery try")

        servers_struct = services.hetzner.servers.list(query=f"cluster:{cluster.name}")
        server_machines = servers_struct.objects_list

        server_machines = [machine for machine in server_machines if machine.state in models.machine.STATES_UP]

        if cluster.size_has != len(server_machines):
            # adjust cluster value
            cluster.size_has = len(server_machines)

            db_session.add(cluster)
            db_session.commit()

            logger.info(f"[workq-scaler] cluster '{cluster.name}' request {request.id} state '{request.state}' ask {request.size_ask} has {cluster.size_has} - recovery changes")

    return 0


cli.add_command(run)

if __name__ == "__main__":
    asyncio.run(cli())